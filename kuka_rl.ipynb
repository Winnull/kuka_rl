{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/monster/anaconda3/envs/python36/lib/python3.6/site-packages/pybullet_envs/bullet\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import timeit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KukaDiverseObjectEnv(renders=True, isDiscrete=False, removeHeightHack=True, maxSteps=20)\n",
    "env.cid = p.connect(p.DIRECT)\n",
    "action_space = spaces.Box(low=-1, high=1, shape=(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-Critic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_hidden_layer(input_dim, hidden_layers):\n",
    "    \"\"\"Build hidden layer.\n",
    "    Params\n",
    "    ======\n",
    "        input_dim (int): Dimension of hidden layer input\n",
    "        hidden_layers (list(int)): Dimension of hidden layers\n",
    "    \"\"\"\n",
    "    hidden = nn.ModuleList([nn.Linear(input_dim, hidden_layers[0])])\n",
    "    if len(hidden_layers)>1:\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        hidden.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "    return hidden\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,state_size,action_size,channel_size,\n",
    "                 critic_hidden_layers=[],actor_hidden_layers=[],\n",
    "                 seed=0, init_type=None):\n",
    "        \"\"\"Initialize parameters and build policy.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            shared_layers (list(int)): Dimension of the shared hidden layers\n",
    "            critic_hidden_layers (list(int)): Dimension of the critic's hidden layers\n",
    "            actor_hidden_layers (list(int)): Dimension of the actor's hidden layers\n",
    "            seed (int): Random seed\n",
    "            init_type (str): Initialization type\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.init_type = init_type\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.sigma = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "        #Added shared layers for the pixel input\n",
    "        self.feature_dim = 512\n",
    "        self.conv1 = layer_init(nn.Conv2d(channel_size, 32, kernel_size=8, stride=4))\n",
    "        self.conv2 = layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n",
    "        self.conv3 = layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1))\n",
    "        self.fc4 = layer_init(nn.Linear(7 * 7 * 64, self.feature_dim))\n",
    "        \n",
    "        \n",
    "        # Add critic layers\n",
    "        if critic_hidden_layers:\n",
    "            # Add hidden layers for critic net if critic_hidden_layers is not empty\n",
    "            self.critic_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                    hidden_layers=critic_hidden_layers)\n",
    "            self.critic = nn.Linear(critic_hidden_layers[-1], 1)\n",
    "        else:\n",
    "            self.critic_hidden = None\n",
    "            self.critic = nn.Linear(64, 1)\n",
    "\n",
    "        # Add actor layers\n",
    "        if actor_hidden_layers:\n",
    "            # Add hidden layers for actor net if actor_hidden_layers is not empty\n",
    "            self.actor_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                   hidden_layers=actor_hidden_layers)\n",
    "            self.actor = nn.Linear(actor_hidden_layers[-1], action_size)\n",
    "        else:\n",
    "            self.actor_hidden = None\n",
    "            self.actor = nn.Linear(64, action_size)\n",
    "\n",
    "        # Apply Tanh() to bound the actions\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Initialize hidden and actor-critic layers\n",
    "        if self.init_type is not None:\n",
    "            self.fc4.apply(self._initialize)\n",
    "            self.critic.apply(self._initialize)\n",
    "            self.actor.apply(self._initialize)\n",
    "            if self.critic_hidden is not None:\n",
    "                self.critic_hidden.apply(self._initialize)\n",
    "            if self.actor_hidden is not None:\n",
    "                self.actor_hidden.apply(self._initialize)\n",
    "\n",
    "    def _initialize(self, n):\n",
    "        \"\"\"Initialize network weights.\n",
    "        \"\"\"\n",
    "        if isinstance(n, nn.Linear):\n",
    "            if self.init_type=='xavier-uniform':\n",
    "                nn.init.xavier_uniform_(n.weight.data)\n",
    "            elif self.init_type=='xavier-normal':\n",
    "                nn.init.xavier_normal_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-uniform':\n",
    "                nn.init.kaiming_uniform_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-normal':\n",
    "                nn.init.kaiming_normal_(n.weight.data)\n",
    "            elif self.init_type=='orthogonal':\n",
    "                nn.init.orthogonal_(n.weight.data)\n",
    "            elif self.init_type=='uniform':\n",
    "                nn.init.uniform_(n.weight.data)\n",
    "            elif self.init_type=='normal':\n",
    "                nn.init.normal_(n.weight.data)\n",
    "            else:\n",
    "                raise KeyError('initialization type is not found in the set of existing types')\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> (action, value).\"\"\"\n",
    "        def apply_multi_layer(layers,x,f=F.leaky_relu):\n",
    "            for layer in layers:\n",
    "                x = f(layer(x))\n",
    "            return x\n",
    "\n",
    "        y = F.relu(self.conv1(state))\n",
    "        y = F.relu(self.conv2(y))\n",
    "        y = F.relu(self.conv3(y))\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = F.relu(self.fc4(y))\n",
    "        state = y\n",
    "\n",
    "        v_hid = state\n",
    "        if self.critic_hidden is not None:\n",
    "            v_hid = apply_multi_layer(self.critic_hidden,v_hid)\n",
    "\n",
    "        a_hid = state\n",
    "        if self.actor_hidden is not None:\n",
    "            a_hid = apply_multi_layer(self.actor_hidden,a_hid)\n",
    "\n",
    "        a = self.tanh(self.actor(a_hid))\n",
    "        value = self.critic(v_hid).squeeze(-1)\n",
    "        return a, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "There are (48, 48, 3) agents. Each observes a state with length: 48\n",
      "The state for the agent looks like: AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reset the environment\n",
    "states = env.reset()\n",
    "num_agents = 1\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape, state_size))\n",
    "print('The state for the agent looks like:', plt.imshow(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5, train_mode=False):\n",
    "\n",
    "    def to_tensor(x, dtype=np.float32):\n",
    "        x = torch.from_numpy(x)\n",
    "        print(x.shape)\n",
    "        x = x.permute(2,0,1) \n",
    "        print(x.shape)\n",
    "        return torch.from_numpy(np.array(x).astype(dtype)).to(device)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "    value_list=[]\n",
    "    done_list=[]\n",
    "\n",
    "    env_info = envs.reset()\n",
    "\n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        action = np.random.randn(1, action_size)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    for t in range(tmax):\n",
    "        next_state=env_info\n",
    "        states = to_tensor(next_state)\n",
    "        action_est, values = policy(states.reshape(1,3,48,48))\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        log_probs = torch.sum(log_probs, dim=-1).detach()\n",
    "        values = values.detach()\n",
    "        actions = actions.detach()\n",
    "\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        rewards = to_tensor(env_info.rewards)\n",
    "        dones = to_tensor(env_info.local_done, dtype=np.uint8)\n",
    "\n",
    "        state_list.append(states.unsqueeze(0))\n",
    "        prob_list.append(log_probs.unsqueeze(0))\n",
    "        action_list.append(actions.unsqueeze(0))\n",
    "        reward_list.append(rewards.unsqueeze(0))\n",
    "        value_list.append(values.unsqueeze(0))\n",
    "        done_list.append(dones.unsqueeze(0))\n",
    "        #if np.any(dones.cpu().numpy()):\n",
    "        if np.any(dones.numpy()):\n",
    "            env_info = envs.reset()\n",
    "\n",
    "    state_list = torch.cat(state_list, dim=0)\n",
    "    prob_list = torch.cat(prob_list, dim=0)\n",
    "    action_list = torch.cat(action_list, dim=0)\n",
    "    reward_list = torch.cat(reward_list, dim=0)\n",
    "    value_list = torch.cat(value_list, dim=0)\n",
    "    done_list = torch.cat(done_list, dim=0)\n",
    "    return prob_list, state_list, action_list, reward_list, value_list, done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(rewards, values, dones):\n",
    "    n_step = len(rewards)\n",
    "    n_agent = len(rewards[0])\n",
    "\n",
    "    # Create empty buffer\n",
    "    GAE = torch.zeros(n_step,n_agent).float().to(device)\n",
    "    returns = torch.zeros(n_step,n_agent).float().to(device)\n",
    "\n",
    "    # Set start values\n",
    "    GAE_current = torch.zeros(n_agent).float().to(device)\n",
    "\n",
    "    TAU = 0.95\n",
    "    discount = 0.99\n",
    "    values_next = values[-1].detach()\n",
    "    returns_current = values[-1].detach()\n",
    "    for irow in reversed(range(n_step)):\n",
    "        values_current = values[irow]\n",
    "        rewards_current = rewards[irow]\n",
    "        gamma = discount * (1. - dones[irow].float())\n",
    "\n",
    "        # Calculate TD Error\n",
    "        td_error = rewards_current + gamma * values_next - values_current\n",
    "        # Update GAE, returns\n",
    "        GAE_current = td_error + gamma * TAU * GAE_current\n",
    "        returns_current = rewards_current + gamma * returns_current\n",
    "        # Set GAE, returns to buffer\n",
    "        GAE[irow] = GAE_current\n",
    "        returns[irow] = returns_current\n",
    "\n",
    "        values_next = values_current\n",
    "\n",
    "    return GAE, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "An actor-critic structure with continuous action space is used for this project. The policy consists of 3 parts, a shared hidden layers, actor, and critic.\n",
    "The actor layer outputs the mean value of a normal distribution, from which the agent's action is sampled. The critic layer yields the value function.\n",
    "\n",
    "- Shared layer:\n",
    "```\n",
    "Input State(33) -> Dense(128) -> LeakyReLU -> Dense(128) -> LeakyReLU*\n",
    "```\n",
    "- Actor and Critic layers:\n",
    "```\n",
    "LeakyRelu* -> Dense(64) -> LeakyRelu -> Dense(4)-> tanh -> Actor's output\n",
    "LeakyReLU* -> Dense(64) -> LeakyRelu -> Dense(1) -> Critic's output\n",
    "```\n",
    "\n",
    "### Model update using PPO/GAE\n",
    "The hyperparameters used during training are:\n",
    "\n",
    "Parameter | Value | Description\n",
    "------------ | ------------- | -------------\n",
    "Number of Agents | 20 | Number of agents trained simultaneously\n",
    "Episodes | 2000 | Maximum number of training episodes\n",
    "tmax | 1000 | Maximum number of steps per episode\n",
    "Epochs | 10 | Number of training epoch per batch sampling\n",
    "Batch size | 128*20 | Size of batch taken from the accumulated  trajectories\n",
    "Discount (gamma) | 0.99 | Discount rate \n",
    "Epsilon | 0.1 | Ratio used to clip r = new_probs/old_probs during training\n",
    "Gradient clip | 10.0 | Maximum gradient norm \n",
    "Beta | 0.01 | Entropy coefficient \n",
    "Tau | 0.95 | tau coefficient in GAE\n",
    "Learning rate | 2e-4 | Learning rate \n",
    "Optimizer | Adam | Optimization method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=ActorCritic(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              channel_size=3,\n",
    "              critic_hidden_layers=[],\n",
    "              actor_hidden_layers=[],\n",
    "              init_type='xavier-uniform',\n",
    "              seed=0).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 48, 3])\n",
      "torch.Size([3, 48, 48])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 256], m2: [3136 x 512] at /opt/conda/conda-bld/pytorch_1573049306851/work/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-9713d92e264f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                                                        \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                                        \u001b[0mnrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                                                                        train_mode=True)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mavg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-479704aa9565>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(envs, policy, tmax, nrand, train_mode)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maction_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-09bea58cd4fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 256], m2: [3136 x 512] at /opt/conda/conda-bld/pytorch_1573049306851/work/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "discount = 0.993\n",
    "epsilon = 0.07\n",
    "beta = .01\n",
    "opt_epoch = 10\n",
    "episode = 2000\n",
    "batch_size = 1\n",
    "tmax = 10 #env episode steps\n",
    "save_scores = []\n",
    "print_per_n = min(10,episode/10)\n",
    "counter = 0\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for e in range(episode):\n",
    "    policy.eval()\n",
    "    old_probs_lst, states_lst, actions_lst, rewards_lst, values_lst, dones_list = collect_trajectories(envs=env,\n",
    "                                                                                                       policy=policy,\n",
    "                                                                                                       tmax=tmax,\n",
    "                                                                                                       nrand = 0,\n",
    "                                                                                                       train_mode=True)\n",
    "\n",
    "    avg_score = rewards_lst.sum(dim=0).mean().item()\n",
    "    scores_window.append(avg_score)\n",
    "    save_scores.append(avg_score)\n",
    "    \n",
    "    gea, target_value = calc_returns(rewards = rewards_lst,\n",
    "                                     values = values_lst,\n",
    "                                     dones=dones_list)\n",
    "    gea = (gea - gea.mean()) / (gea.std() + 1e-8)\n",
    "\n",
    "    policy.train()\n",
    "\n",
    "    # cat all agents\n",
    "    def concat_all(v):\n",
    "        if len(v.shape) == 3:\n",
    "            return v.reshape([-1, v.shape[-1]])\n",
    "        return v.reshape([-1])\n",
    "\n",
    "    old_probs_lst = concat_all(old_probs_lst)\n",
    "    states_lst = concat_all(states_lst)\n",
    "    actions_lst = concat_all(actions_lst)\n",
    "    rewards_lst = concat_all(rewards_lst)\n",
    "    values_lst = concat_all(values_lst)\n",
    "    gea = concat_all(gea)\n",
    "    target_value = concat_all(target_value)\n",
    "\n",
    "    # gradient ascent step\n",
    "    n_sample = len(old_probs_lst)//batch_size\n",
    "    idx = np.arange(len(old_probs_lst))\n",
    "    np.random.shuffle(idx)\n",
    "    for epoch in range(opt_epoch):\n",
    "        for b in range(n_sample):\n",
    "            ind = idx[b*batch_size:(b+1)*batch_size]\n",
    "            g = gea[ind]\n",
    "            tv = target_value[ind]\n",
    "            actions = actions_lst[ind]\n",
    "            old_probs = old_probs_lst[ind]\n",
    "\n",
    "            action_est, values = policy(states_lst[ind])\n",
    "            sigma = nn.Parameter(torch.zeros(action_size))\n",
    "            dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            log_probs = torch.sum(log_probs, dim=-1)\n",
    "            entropy = torch.sum(dist.entropy(), dim=-1)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_probs)\n",
    "            ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            L_CLIP = torch.mean(torch.min(ratio*g, ratio_clipped*g))\n",
    "            # entropy bonus\n",
    "            S = entropy.mean()\n",
    "            # squared-error value function loss\n",
    "            L_VF = 0.5 * (tv - values).pow(2).mean()\n",
    "            # clipped surrogate\n",
    "            L = -(L_CLIP - L_VF + beta*S)\n",
    "            optimizer.zero_grad()\n",
    "            # This may need retain_graph=True on the backward pass\n",
    "            # as pytorch automatically frees the computational graph after\n",
    "            # the backward pass to save memory\n",
    "            # Without this, the chain of derivative may get lost\n",
    "            L.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            del(L)\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.998\n",
    "    \n",
    "    # display some progress every n iterations\n",
    "    if (e+1)%print_per_n ==0 :\n",
    "        print(\"Episode: {0:d}, average score: {1:.2f}\".format(e+1,np.mean(scores_window)), end=\"\\n\")\n",
    "    else:\n",
    "        print(\"Episode: {0:d}, score: {1:.2f}\".format(e+1, avg_score), end=\"\\r\")\n",
    "    if np.mean(scores_window)<5.0:\n",
    "        counter = 0# stop if any of the trajectories is done to have retangular lists\n",
    "    if e>=25 and np.mean(scores_window)>30.0:\n",
    "        print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e+1, np.mean(scores_window)))\n",
    "        break\n",
    "\n",
    "\n",
    "print('Average Score: {:.2f}'.format(np.mean(scores_window)))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time: {}\".format(timedelta(seconds=elapsed)))\n",
    "print(\"Saving checkpoint!\")\n",
    "# save your policy!\n",
    "torch.save(policy.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(envs, policy, tmax=1000):\n",
    "    reward_list=[]\n",
    "    env_info = envs.reset(train_mode=False)[brain_name]\n",
    "    for t in range(tmax):\n",
    "        states = torch.from_numpy(env_info.vector_observations).float().to(device)\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        reward = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        reward_list.append(np.mean(reward))\n",
    "\n",
    "        # stop if any of the trajectories is done to have retangular lists\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "      #print(state)\n",
    "      old_Q = Q[state][tuple(np.round(actions[i].flat))]\n",
    "      Q[state][tuple(np.round(actions[i].flat))] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA, height_hack_prob=0.9):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "      env.render(mode='human')\n",
    "      state_m = tuple(np.round(state.flat))\n",
    "      action = np.random.choice(np.arange(nA), p=get_probs(Q[state_m], epsilon, nA)) if state_m in Q else action_space.sample()\n",
    "      if np.random.random() < height_hack_prob:\n",
    "        action[2] = -1\n",
    "      if state_m in Q:\n",
    "        print(\"*******************From Q!*************************\") \n",
    "      action_r = np.round(action)\n",
    "      next_state, reward, done, info = env.step(action_r)\n",
    "      episode.append((state_m, action_r, reward))\n",
    "      state = next_state\n",
    "      if done:\n",
    "        break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=0.95, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    nA = action_space.shape[0]\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: np.zeros(nA)))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(episode, Q, alpha, gamma)\n",
    "        # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = mc_control(env, num_episodes=3, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
