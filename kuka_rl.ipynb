{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=C:\\Users\\ecl23226\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pybullet_envs\\bullet\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KukaDiverseObjectEnv(renders=True, isDiscrete=False, removeHeightHack=True, maxSteps=20)\n",
    "env.cid = p.connect(p.DIRECT)\n",
    "action_space = spaces.Box(low=-1, high=1, shape=(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA, height_hack_prob=0.9):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "      env.render(mode='human')\n",
    "      state_m = tuple(np.round(state.flat))\n",
    "      action = np.random.choice(np.arange(nA), p=get_probs(Q[state_m], epsilon, nA)) if state_m in Q else action_space.sample()\n",
    "      if np.random.random() < height_hack_prob:\n",
    "        action[2] = -1\n",
    "      if state_m in Q:\n",
    "        print(\"*******************From Q!*************************\") \n",
    "      action_r = np.round(action)\n",
    "      next_state, reward, done, info = env.step(action_r)\n",
    "      episode.append((state_m, action_r, reward))\n",
    "      state = next_state\n",
    "      if done:\n",
    "        break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "      #print(state)\n",
    "      old_Q = Q[state][tuple(np.round(actions[i].flat))]\n",
    "      Q[state][tuple(np.round(actions[i].flat))] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=0.95, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    nA = action_space.shape[0]\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: np.zeros(nA)))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(episode, Q, alpha, gamma)\n",
    "        # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = mc_control(env, num_episodes=3, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
