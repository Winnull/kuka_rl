{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import timeit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KukaDiverseObjectEnv(renders=True, isDiscrete=False, removeHeightHack=True, maxSteps=20)\n",
    "env.cid = p.connect(p.DIRECT)\n",
    "action_space = spaces.Box(low=-1, high=1, shape=(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-Critic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_hidden_layer(input_dim, hidden_layers):\n",
    "    \"\"\"Build hidden layer.\n",
    "    Params\n",
    "    ======\n",
    "        input_dim (int): Dimension of hidden layer input\n",
    "        hidden_layers (list(int)): Dimension of hidden layers\n",
    "    \"\"\"\n",
    "    hidden = nn.ModuleList([nn.Linear(input_dim, hidden_layers[0])])\n",
    "    if len(hidden_layers)>1:\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        hidden.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "    return hidden\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,state_size,action_size,shared_layers,\n",
    "                 critic_hidden_layers=[],actor_hidden_layers=[],\n",
    "                 seed=0, init_type=None):\n",
    "        \"\"\"Initialize parameters and build policy.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            shared_layers (list(int)): Dimension of the shared hidden layers\n",
    "            critic_hidden_layers (list(int)): Dimension of the critic's hidden layers\n",
    "            actor_hidden_layers (list(int)): Dimension of the actor's hidden layers\n",
    "            seed (int): Random seed\n",
    "            init_type (str): Initialization type\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.init_type = init_type\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.sigma = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "        # Add shared hidden layer\n",
    "        self.shared_layers = build_hidden_layer(input_dim=state_size,\n",
    "                                                hidden_layers=shared_layers)\n",
    "\n",
    "        # Add critic layers\n",
    "        if critic_hidden_layers:\n",
    "            # Add hidden layers for critic net if critic_hidden_layers is not empty\n",
    "            self.critic_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                    hidden_layers=critic_hidden_layers)\n",
    "            self.critic = nn.Linear(critic_hidden_layers[-1], 1)\n",
    "        else:\n",
    "            self.critic_hidden = None\n",
    "            self.critic = nn.Linear(shared_layers[-1], 1)\n",
    "\n",
    "        # Add actor layers\n",
    "        if actor_hidden_layers:\n",
    "            # Add hidden layers for actor net if actor_hidden_layers is not empty\n",
    "            self.actor_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                   hidden_layers=actor_hidden_layers)\n",
    "            self.actor = nn.Linear(actor_hidden_layers[-1], action_size)\n",
    "        else:\n",
    "            self.actor_hidden = None\n",
    "            self.actor = nn.Linear(shared_layers[-1], action_size)\n",
    "\n",
    "        # Apply Tanh() to bound the actions\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Initialize hidden and actor-critic layers\n",
    "        if self.init_type is not None:\n",
    "            self.shared_layers.apply(self._initialize)\n",
    "            self.critic.apply(self._initialize)\n",
    "            self.actor.apply(self._initialize)\n",
    "            if self.critic_hidden is not None:\n",
    "                self.critic_hidden.apply(self._initialize)\n",
    "            if self.actor_hidden is not None:\n",
    "                self.actor_hidden.apply(self._initialize)\n",
    "\n",
    "    def _initialize(self, n):\n",
    "        \"\"\"Initialize network weights.\n",
    "        \"\"\"\n",
    "        if isinstance(n, nn.Linear):\n",
    "            if self.init_type=='xavier-uniform':\n",
    "                nn.init.xavier_uniform_(n.weight.data)\n",
    "            elif self.init_type=='xavier-normal':\n",
    "                nn.init.xavier_normal_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-uniform':\n",
    "                nn.init.kaiming_uniform_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-normal':\n",
    "                nn.init.kaiming_normal_(n.weight.data)\n",
    "            elif self.init_type=='orthogonal':\n",
    "                nn.init.orthogonal_(n.weight.data)\n",
    "            elif self.init_type=='uniform':\n",
    "                nn.init.uniform_(n.weight.data)\n",
    "            elif self.init_type=='normal':\n",
    "                nn.init.normal_(n.weight.data)\n",
    "            else:\n",
    "                raise KeyError('initialization type is not found in the set of existing types')\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> (action, value).\"\"\"\n",
    "        def apply_multi_layer(layers,x,f=F.leaky_relu):\n",
    "            for layer in layers:\n",
    "                x = f(layer(x))\n",
    "            return x\n",
    "\n",
    "        state = apply_multi_layer(self.shared_layers,state)\n",
    "\n",
    "        v_hid = state\n",
    "        if self.critic_hidden is not None:\n",
    "            v_hid = apply_multi_layer(self.critic_hidden,v_hid)\n",
    "\n",
    "        a_hid = state\n",
    "        if self.actor_hidden is not None:\n",
    "            a_hid = apply_multi_layer(self.actor_hidden,a_hid)\n",
    "\n",
    "        a = self.tanh(self.actor(a_hid))\n",
    "        value = self.critic(v_hid).squeeze(-1)\n",
    "        return a, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "There are (48, 48, 3) agents. Each observes a state with length: 48\n",
      "The state for the agent looks like: AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deZydZZXnf+futypVqWwklVQgASIYWZt0lNFu/Wjbg0iLo4yjrSM4OLTdauOIAgIuSKPo2CAO2jOISro/jmC7QYOiSIMOtsMuEBJCdrJUFpJUllru+swfdcPk+Z1TuZcsNwXv+X4+fMjz1Hnf97nvfZ976/zqLBJCgOM4r3xSR3oBjuO0B9/sjpMQfLM7TkLwze44CcE3u+MkBN/sjpMQDmqzi8hZIrJMRFaIyOWHalGO4xx65ED/zi4iaQDPAXgrgPUAHgHwvhDCkrGOmdjVEaZP6Wl25ham9JqFjUR/jkkL5+GL6WP0YZLS1wqhblw/RWPj5K28H3yYcUjg22Gdhq9v3o4WboCx5i0Dw8ZxMSnjvh0Kpk4sqjnzXhPrtteM4+g9M9bMNpOHViqbNF0+ZSyH5+z3jI6hn6/bE7BthN/9UTLWZIssBLAihLBqdBFyG4BzAYy52adP6cH/uOrCeJJuVArp5qs0NlI6nY3H2bw+TSa+B/WaPg/f8UxarydFz3amoK9Vr5X1GjO5aJyjNQNArVbhq+nrp2nddb3GmsSL5IfNWo/UtVHaOLCGeFOEqt4kN/3sD/qCRLGoN+Wh4MPnnKrmMtn4Xovxwfbfbtutj8vHa8zlO7RNLrZ572PvUjaTC/G4I6Pva4HexryxFXI0l6Pn9c/vGNEHNTiYj9ZZANbtM17fmHMcZxxy2AU6EblIRB4VkUd37h483JdzHGcMDmazbwAwe59xX2MuIoRwcwhhQQhhwcSuzoO4nOM4B8PB+OyPAJgnInMxusnfC+Avmx5FPjr7jSlDuQjqI8kQv1KxM1Ova5sQ4pdbN3QMdlFrFUO0ycT+X71mOICm+BSfvFrVfn2NxK98LqdssvRaLZE1HWjdKcMBpPso7BACKJdZQwA+dOW3onEmo7WH009bYFyvPVg6S4buK2saAHDj+7vUXDoTPzMfW7RN22Tj98jytVsR6Fh8szRFNcUT+9EhD3izhxCqIvIxAL8EkAbw3RDCMwd6PsdxDi8H882OEMLPAfz8EK3FcZzDiEfQOU5COKhv9pdKAFBjX7rGPrHxd136+28hq/3YGvmolofK104bf+tUx2X052GdfOQU+8cAKiPa1y3m43NVDF87R3+zLVW0Xy+Z+O/6wfA/U6aPHvOBS2+Kxp2d+m/INSMW4T3/MZZmLM1gzZo1Ta/fCv/0T7c2teGX/9fvPl3Z1Crx60hl9aNvhaJUSLO5/gMTlc1vrvxgfO5ufSLlj+tL7c/dHpOXcox/sztOQvDN7jgJwTe74yQE3+yOkxDaKtAJgJTElxQl7mjJIUNiU8UIqskJ2VjCVo0y2nIFZZOjhIk6tPjGARJWAE/aCOyop+mz1ThueGRPNM7mdZLNCCXZ/PXnblE2xWJ8XKVWVTYdxViQq9W10HbKyaepOc5WW7lSZ3mNjMQJGXPmzFE2v/hF/FfbUkkncbQiNPIj9KfnfVrZ/Oa2rzY9T84IDipVStE4LXo9xQPYRS0k4ZkJkEbqVsv4N7vjJATf7I6TEHyzO05CaKvPDmiPvE6fN1Uj8aSj2DxZRshHFiOsJpOKzxNSlgcUz1nFW+r0EZlO69tYK5XUHAefpI3X8ZFrFkXjvOGzM+msfq2VKvvoVmkUHmobq5rM8uXLm9qceOKJ0Zh9eED7rTu2b1c27LdaFYCyFGTF49HjAo31czZS1roGI6Lf61b87xaK+6hnzXr2+DiWWfZX6Mi/2R0nIfhmd5yE4JvdcRKCb3bHSQjtDapJCXKFOHBhZCQWsrKG2MViUz6lgx9Yt0kbKUylclzeOG2cp5JnAUgrHoFUkUpFCztp0QLQp677frNTo7s7rpZSNc5dq9O5jfNk87FIVatqYYtFM0vYaoVdu3apuZ/+7EfR+G1nnWMcGa9pcMgoP81CmlEiXOuK+rV+9qZ4Pdf+7XnKxgqE4nttBVDx/Tdik9SKWhHfDlTEGwv/ZnechOCb3XESgm92x0kIbQ6qEfDnS64QV2YpGwksearwWVIBI0CG3K1MTvvjmSoZGYEVNaoUmzWq2Vz2jZ9E44pRgZWDfEavp6eYUiU+V8royMJdcyyb4ZH4PuZz+q0u5CkRyAgOsXzUrq5YV3hmyVMtHcdUSUeosxYB3VrpTX9yprJhnUMnVwH33v+7aHzWBYuVzd3f+ayay1G3n6pRuafags/OBYitZkTckMc6jzrmJbRv8292x0kIvtkdJyH4ZnechOCb3XESQntLSYeAKrckpsofWaNaSKBy0/mCtlHthg3hIkU1h6+46Q59HkphqqlS10ChEAtbdSN7rlzWQmOKKtWkrAAR6nVliVaBlL46p+EByJGoaell9XosdHILLcDua87BOIODQ/rkdNymTZuUyTnnvCMar139rLLh6jU5K6OthaiWNJUEt4StlCGqsohoZfiNkF5sdQNjQa7Gfb8B1CgQzDBR5+HEyf3Jdf7N7jgJwTe74yQE3+yOkxDaXqmG2ySzH50yPn8q5LfmDD/ysm/8MLYxKryUqHqMVbm0VuUAGX0t9seD4VdbSRUcIFI3qq6kKUCmbvifHLBSMF4r6wOlkhH4Q29/3QhWeuzxR9TcaxfGgS3HHd2nbCZ0dcbrMXzdyZMnR+OOjj9SNksXP950jULPjBXOE1jTMVpmnf2hz6u5u7/3hWicNl7Hn1373Wj81LX/RdmwP86BOACQoTnLZ38plWkY/2Z3nITgm91xEoJvdsdJCL7ZHSchHPFS0mkSrT594+3qmCJlxlmkU/FLKVuCFAXw1I3UIw7QSBntfvSJrc9MfW4WqYIRfVGnIB5bxIuDiioV/VpR4TvdvOxJ2RC/Tp0/T80N7toajTtJjAPsYJxmNhysBOjS0YuXLFE2NWptNWvWbH0eauMlVsGZlN4O//6DsWj36/99rbJhEbWs9VpQe3jkjOtXSTS07iE/Mq1kxu3Fv9kdJyH4ZnechNB0s4vId0Vki4gs3mdusojcKyLLG/+fdHiX6TjOwdKKz34rgJsA/OM+c5cDuC+EcJ2IXN4YX9bsRBu3DuDzN98ZT5LPUchr/7zGCRtGoAsHF4gR/FCvx76t2RKXAlbq0H5sjiq31g3/nAM9AKBKvmUmo28/x+ewpgEAKfLl6pZ/TK9t+tQeZdLVFbds5iQcwPYbuZWSxbZt2/hEyubZZ+PEl1e96lX6RC20beL2W08vWaZs+F5bwUpWFRp+jLiSEaBbeJda8NmNYr/gjt5Wd7Iar+dQtn8KIfwWADfhOhfA3qZkiwC8s9l5HMc5shyozz49hNDf+PcmANMP0XocxzlMHPSf3kIIQcQING4gIhcBuAgAcrkDa0LgOM7Bc6Df7JtFpBcAGv/fMpZhCOHmEMKCEMKCbLbtf9Z3HKfBge6+OwGcD+C6xv91yRcTUYEtqh96C+KPqkwCLSSZmhWLGda5uY0UqybQwTiWGGfBgT9WSyah0iPBKDEzbWqcLTaRSinvXdX+zgvotk0Tu7uVDQf5ADpb0FpjT08sCK5ZvVrZzD95SjQeGBhQNstXrIzGJ510kl4jXX7r7x5SNpzxaGUlWqW0A4mWb79Al5u+Z1EcaPPaq25WNou/clE0LhvvR5qfPeMX5ipl71EBnoOrVCMiPwDwewAniMh6EbkQo5v8rSKyHMCfNcaO44xjmn6zhxDeN8aP3nKI1+I4zmHEI+gcJyG0XTHjgBAOUhCrPIcKmGmhta6RCKMFQn2tbGcc1GNpCNxuychVQbVqVIWlc804ygh0mTCBrmW81lrzcqYp8glrxnomdsU++q5du5VNt+HHr1i+PBofe9yxymbzRl1NthlWy6iFZ74xGj/5xO+VzYze3mhsBVSBtAcroIlbcQNaVwlZ7Wv/+flXRuN7vneNshmh2583JIOM8tm1TYme2Yzh+4+Ff7M7TkLwze44CcE3u+MkBN/sjpMQ2l9KmkSYIM2zmlIc7WAoYvypVTNsSqW4BDRnrwE6M80StjjLygrgmXPMDD2pzqNVGp6zSmuzQJfJ63ZYQ9SSyep7v33Hjmh89Gxd4eWFrVvVHL/gtavWKJM5x86Nxq30a7dabXV0xJl502fo+6oDoSzRKr5HNSPDzTpK6Fw143Ww2Ccc6QJgD93+YlqLgSy2ZYwF8ak5Luugst4cx3ll4JvdcRKCb3bHSQi+2R0nIbS/lDSVWcrQ2BJAdF8urVzUSdjLGn28WbSzBDIWbmb1TlM2xSKVPG5eFWr0emRolZxS66nrsljcb2zzZiNajRYwbbquL5LPxT3idu/Zo2ymTtOvv2dyXHLQ6pnHt8Tsq2cIcgxHHVoC1MOP/CEaW73X+X5kDBGNe7EDQI6iLkdKWujkctdv++BVyuaef7w1Gj/0hQuUDS/JEuh0ZhytRR/yIv7N7jgJwTe74yQE3+yOkxDa7rOzD1ZTQTRGsEE69i2rNe03pSQOmrCCc46aEmdwTZw4sen6SsPDyoYDbzjIBgCyVuYV+VdcShkATnj1idHY8j/Z1x3eo9dI7cCxZvVKZdM7M+6r3lnsUDZWJtj69RuicY+RGVfsjFtC7TKq0OxZE1ev6Vz4J8pm586d0bhS1u+9qu5jdMPKZuPno1oz2oMZb1mVs+WyVrltOsbw/c8+P65wc8ksfa18qnlGW4oea6H9sr92UP7N7jgJwTe74yQE3+yOkxB8sztOQmirQBcQUAuxuJUClSU2+qaxIDVtkhaE8sW4nJQVsMLCWv+GjcpmZt/MaLx23fPK5oQTTqAZrYpYOsmqlauicSZjBKNQUNHaVVpYKxRiIW1mX6+y2bJ5czSeMaNP2TxH5z527lxlU6tqIWvWTFaX9KvdQCIe31cA6OyMS3ANDg4qm8ceictQVbkZHqAibaxgqToFJ6VVSXMtxgFAOkvPpyH8VigzUoyeeSxEv/GLtyibh7/44WicMUq0pZuUTPegGsdxfLM7TlLwze44CUFaabd0qOjs7AyvOWl+NDdtUuy3WaWLGavqCbdp6t+kk0O4ysmO7dyJGuglm0pF+2jDw7FvOWGCbr80NDyk5rZtjlvicZUeQCeMWP5nivSIbCGvbIYG40AbK8ZnRm/sR68z9Am+FgDM7Iv9f+sJ6u+P779VprpSifUALn8NAEKazusWnq5s7vn1b6JxJqsr9wTy9WtVveqccVyFgm84OAcAivn4/u/crROKWFfIGG3FPj071pAm6rcVnVTKukiPx988UMaygbpZX9q/2R0nIfhmd5yE4JvdcRKCb3bHSQhtDarJ5zKYM/uoaI71wbpVApoynTb3a/Gtl/p9TT/qKGWTy8Tiyu6Bncpmx/a4vPLsPh2MspkCVjZu7Fc2GdVXDshRyWHu8w5oscvOYqKecTN0UM3KlXHAzIjR+279unV0Wq3rdE2apOaWPvtc80UqsU1/r2QzsQL1uj8+RdlUSEizSkB3UEAV92IHAJH43mcyes38nAGAUKpiua7vY2kkvp4l4nHvP6sg01efj4OVrpy7QdlkKdCGg2z2V7Dbv9kdJyH4ZnechOCb3XESQnsr1YggxQkIHMhvVWapxkkMVlDN8+R/zjv+eGWzbNmyaGx90s0+Om6BlM1o/4sDX8Q40zHHzFFz659fE43Tom+/8kkNd5hf/aoVy5WNCs5J6WvtGY59zWBcbHCD1Wc9fr11aD/2tWecQTNaD6hSUI1VKmbV6lgfmNitqwv9yb9bGI0f+K3u4c63tVrVVXt1FWMgReVbrTwcTtSyAma4Am3deK2BFvmGq29VNo988YJonFbt0/T69uLf7I6TEHyzO05C8M3uOAmh6WYXkdkicr+ILBGRZ0Tk4sb8ZBG5V0SWN/6v/yDrOM64oRWBrgrgkhDC4yLSBeAxEbkXwAUA7gshXCcilwO4HMBl+z1RpYotW+J+332z4kCCkYoOiKhT0IZV3pjlnzVUphgAurrj7LRdu3YpmwJlMFkCXZb6us+aqauwrFyhK8wcT6LhypUr9LlTHHijhSQhkbNuNIjn4KA8t6yCIXQa4s7pp71GzaWpTxG/P4COszG0L2zaGGd5DZdGlM1xx8+LxlxtCNC952f2TlU2G/tfiMbB6K0koluG8T1JZYxAqEC9343Xym3PrOgX7vN+9gVXKptLY/0YXNn6oCrVhBD6QwiPN/69G8BSALMAnAtgUcNsEYB3NjuX4zhHjpf0pzcRmQPgdAAPAZgeQtgbJ7oJgO4cOHrMRQAuAoB83vjkdBynLbQs0InIBAA/BvCJEEL0+28Y/eO4HcUdws0hhAUhhAVWcQDHcdpDS9/sIpLF6Eb/fgjhJ43pzSLSG0LoF5FeAFvGPsMo6XQaPT1xUESZfPQVy7Wvy1VW5r/61cpm+fLY/509+2hlUyN/b2RI+4jpdPyBtHLVKmVz9DHHRONVq7Q+ACOwYtXq2C4YgRVCFWeHS0alnEGq+mKcJ0U3rWwkwiykxJNgFDgxOi2rJI5Nm3QiUKEQJ6f09PQom97ZcZJRtaL9cU6MWvLMUmVDOSbIZI3qsmDdR5nAiDtSr7VirFFLJlZQTfO24/oYPffl52N96Oq5se5xUO2fREQAfAfA0hDC9fv86E4A5zf+fT6AO5qdy3GcI0cr3+yvB/CfATwtInu73l8B4DoAPxSRCwGsBfCew7NEx3EOBU03ewjhQYz9O8dbDu1yHMc5XHgEneMkhDZnvQFCwtWy52JBbt7xxzU9TbmsxaZjSDRLZ/RLW7GURTxdhWYFtUTqna7/orhmzZqm57H+NvH0M89E43yuqGxGhuMAkYrR6zufiwNkTj1FC5YcxWKVhBYS5Kxe8OWSrt4ySGWqe2fooCI+U8VorZQhReyZpcuUDf9S2d2lhb7jjoufmZIRnMMv7d9+rzPjLEGM2z2ZraXIpmZkZWb4/huBUJzxaGWAsmA6THqh92d3HMc3u+MkBd/sjpMQ2uqzC3SlmmOoMozVsnn5cm4tPEfZbKD2yx0Fnfgh5Cfl8rq/TpUqjFqthLgyyVNPP6tsCsb1s+Sjl412yCeeeHI0ntChP4/r7P8abZP4PqcMH3HzljgYpjKs/fOZhh4xUorbZi1bvlXZDAzEgT/pjA6VzlIWx9F9OhCKK8pY7cH6++PXMTg0rGw4yWTylCnKZse2bWqO3W9uMwYAGao4ZL2vQs62VW2JlQ5eMwBUq/F79LWN8f7ZXFlvnHcU/2Z3nITgm91xEoJvdsdJCL7ZHSchtDeoBgCLENlcvISaUeZj3rw4aGLFcp2JlqbKI1Nn6PZPR0kcILN6te5HPkRRCk8/rbOsOLAiZQRajBgtiBaccVI8YWVeGQIQw9Vz1q42su4ouoJLIgO6vLQVDMJtpABgYHfce97Smnp6Jkfj6TN0cFKB6htwL3YAWLsuFpwGh/V95WxGvjag37POzgnKprxTtwPbHTjLzajKU49tgnVDDBGV2V8Z6L1wlSIrU3Es/JvdcRKCb3bHSQi+2R0nIfhmd5yE0GaBTpeqq7MgZ5Uc3hSXAe4zorqGh+OoqSVLdAYV90M3gspQpywrzswCgBNeHQttHTpYTpWFAqxMMB2xdvQ9H43Ga9/ydWWzYWPctzub1mvk61t9zbkk9AvbdGntWl2XYTp2zrHRuFjU2Xt7Bgej8dq1WgzlEmCpjBboioWOaNzRqa+V4t5zxjPUSRGNYtS2fsNb3qzm7v75L6OxFUHH0XBWZpy6/1YAHYl4Vj86fl+rFK1nZcq9eOyYP3Ec5xWFb3bHSQi+2R0nIbTVZx8aLuHJp6nfdlfsk1ktmdLkk66ntkEAkM9RVpXZazs+T9bwreadELc76ijo8+zkNea7lE3daOT9HOkIc1dcoI/LxsdN+7W22Tj/y9G4ZGRZjQxzT3tlgmw2Pm7G9Gl6PcaBKfItN/fr9+M180+IxtXykLKp1UgfKWh/nIWVK6atUybXD8RttcQIYJEQ6yPzX/UqZWO1yOICMxzAA2gfvcq1rQEUSTOwgq541TWrrRa9H5wZx5md++Lf7I6TEHyzO05C8M3uOAnBN7vjJATZ3x/hDzXFYjHMnTsnmiuV47K/WaN8UYoiEMpGvy3O6jrllD9SNrl0LEipntkGwTBZtiQuQzXnGF1Oybqv3T2TonHtjjcom97MidGYA0YAoBziAKK06NJZj5x0TTTuNCJ/uG9ZqaKFpVPnz1NzLDZ1U/8+QAf6/K9vv0nZMKcv+Kaau3za2mhslbtmvrbjeDX3qUkrDMuYVnbC+361Xc2l6dmrGAJdmspSVYyAGX5p+ZxRlopKkoUQX3vTpk0olUqmSuff7I6TEHyzO05C8M3uOAmhrT57IZ8PfX290Vyaqq7U99e/psGpp56m5oqZ2JfhNlOj57ayD2LYJ1r3vE7gOH5e7MeupXZQAFCp6ECXNCV67NylSx6fteEb0TgYnuTm6vJovOH0H+hrUQnsgR26CsvsWTOicb2u13zcsdr/nUjlnLn3OQB8+5b4uHxO+/4a7WpefNKHo/GZs3Up6Vb8eHWl/QSf7Mu/PR/ft7XH62SZO+/+VTQ2co4UVnBOymoQr2zidXdNiCvuLF+xAkNDw+6zO06S8c3uOAnBN7vjJATf7I6TENocVFMIc+fEfdTLLfRW4zUuPONkZcOZRtyPDQA2bd4cja1eb1OnxGWI165dq2zSVC1k9uxjlM1io1IOB1+w0AUAfU+9Nx5nTlI2WrTT7+Gzr7shGo8YPctnTIuDfKYY/c+6unRGX3d3HEST5YxDaKntv39NnzufO0HNNeMzp1+k5k6dEYtUlmDHM0u26Cy8qiEOd7330mj80O8fVjZ3/eLX8bWMLWVlwjXHeB00VSWhb+uWrSiXyy7QOU6S8c3uOAmh6WYXkYKIPCwiT4rIMyJydWN+rog8JCIrROR2EdG/yzmOM25o6rPLaPRBZwhhj4hkATwI4GIAnwTwkxDCbSLyPwE8GUL4h/2dq5DPh75ZcVCNcBVWIzmFp1KGzRmnxRVmakalmMWLF0fjXFYnh5x0cuwjP/nU08qGPVKr93mnUXGVq6eeufRaZXNP3yei8Slr/krZzMy8OhpbPe2Zx0/R1zpz4Rm0Pv06Cjl9j7jiz4wZvcqmXI4DhvgYALj27+bGNimtD7DfevFJ/1VZWIE2zWg1EOf6HfEaj+6bpWw6Sdf43qLv6xPxrTUub1U3anYi3j79/ZtQKh2gzx5G2dMYZhv/BQBvBvCjxvwiAO9sYaWO4xwhWvLZRSQtIn8AsAXAvQBWAhgI4cWud+sB6I88x3HGDS1t9hBCLYRwGoA+AAsBnNjkkBcRkYtE5FERedT6c5jjOO3hJanxIYQBAPcDOBNAj4jsdcT6AGwY45ibQwgLQggLrE4ZjuO0h1YEumkAKiGEAREpAvgVgK8AOB/Aj/cR6J4KIXxrf+cqFgvhuOPmRHOc/VM2qqVwH3Gr1fW84+PAlrVrdcnhCl0rZWRZpag0b0dOB97kuOqLEYxRM+5raSQOIJpg9Y2iJU2aNEmZdP/mL6LxjIzOKBv5T7dH42JOC4Ys7PUY1yp2dBrHxa8tm9KBUOVK/Fq/+S1dOeiST8aC6Zev09cq5OfTjH7PLjklDrT541mW0NecViS7n3S/Uc29sGVLNL7vNw8qG86m5AxIQAu9YpVJ4vOSqNff3z9mpZpW6sb3Algko13gUwB+GEK4S0SWALhNRP4OwBMAvtPCuRzHOUI03ewhhKcAnG7Mr8Ko/+44zssAj6BznITQ3ko1hXzoo6AETiqx+ihz9ZpgBB/kKBmjUtHtkLm9UclowcNkjCSPIWpHPHGi9hHLZR3oMo2SbOzj4nV3FjuUTTYb/0JWKGhdoYcq2e7YsU3ZFPOxH58vap/53Av0L3/ctvjhX05VNpVyrI/0TOpRNp//vG7BxIyUYr8+ldL3I5eNA18uO00HIuUoCckqiHTydEufiFmU0trD5InxcWJUnPnBD38WjStGy64c6UNWZaVcPn4eA9msXr0Gw8MjngjjOEnGN7vjJATf7I6TEHyzO05CaGt/9hCal3NmwQHQmXFVo3UOC3RTp+iWRCUW7YzonKGhWLTLVPV6ju6LSzDvHNA95adO1oIUC1uVkhYRuyfEAlTNuB9pyvoLVlWejf3RuDhBC1t81MTZq5RNKtW8mszijfo7I4Q40Oa0Dl0pp1ReruaYL30pfo+uvFK/9+XK6mhsVZz58hM3RuNPnvzxptcGdAjPBfXHlc0N6+KgJjN5jYTngpEVmaHMQCsL7qhpsRga6LWuW2cGsgLwb3bHSQy+2R0nIfhmd5yE0FafHQgIIfZD2P8W6AQB7uSUMZII2LfduXNQ2XBZD6sqajod+5odhm+Vp2CUnh79mdnZqX3kQiE+rqNDn5sDKyzKFAxkVYGZ0B2f26ruk8vFr3WwX7d6stok3Xb7cdF48lQdjLL9hU3R+Mnntfbwjg/qllTMKX8at0g+93xdpfaORXFw0gMFTp4BqtWN0fiGp29SNp9Oaz++K0etlg094OFnHlNzDAdChaDvx2TSeazqx9d/5v3ROEXP63su+tyYa/BvdsdJCL7ZHSch+GZ3nITgm91xEkJbBbp0Oo2J3bEIsXt3HJCSzWrRjMtCj5SNjCHSTSpG3ZGOzlhIsirVZAux4DF1qq7eUqOMpaOmTVM2daNJdyoViz1DQ7o/Oy+7ZATeFEnYswQ6DgfJ53U1GbbpNKrSADozkO/s2pVLlc3M2cfGa0xpUZXFv4HtW5XNLbfEgpyVpfn5z+2OxitXHatsytW7o3FXl36tf/+QmsIV8+PrXbdUXz9Dwm/KKL/GwTBWKesbP/uhaJzO6Pc1Q+ceoZLd+8ti9W92x0kIvtkdJyH4ZnechNDmoBqooBrdglb745VKPJfLaP+Ta9JbwSC5bByk0NGhgxbKdK2hQe1Xzz16ZjS2cnuKXTqopjLMwTD6dZTIRjiiCECN1pgyPrPrIfb1uw0fdYn/3e4AAAzjSURBVNsLL0TjkZJ+rZIyWi3n46q4Z79NJ188/Fh8j6xW3OzHd3bp5CX2W1cvX6xsBnpiDeNff/s7ZbN9YEc8YbxpGeO5um5J/FwdNV1X5UkLBcwY7bhu+OwF0dgKcmI/PtR10s8IBaFxEJgYOtSL1xzzJ47jvKLwze44CcE3u+MkBN/sjpMQ2irQzTxqEq75+LujuY9fe2s0zmd0S6RqNRZJUlx+GkYwgSHQbdsWC1KdE2YrG6HPvxGj3HRpJJ7r7NTiV3lIH8eZeZWyFsSylPVWM6rycMJUva6vJRT68vy6tcqG1833efQ8+j6e964l0fhffrpF2Zx8dHz9HVv7lU3P1FjsemTZHmXTPSm2efiBHyubJx4kYc3QqDJUlUgMMa5nkhYIuXrM16/6kLLRYSx6hkW8mqoTpJ9hq9w1K9qf/Wn8MGwY8KAax0k8vtkdJyH4ZnechOCb3XESQnsj6EJAlfqvTyCRaM8eXU6K9ZaMkQ3Efdu4/DSgy1jv2D6gbIrUW42j9wBg9yCVjhZDojGi6rLUp4vLZAHA4KBRToso0w1JGxllEyZMiC9lfK6XqV98Z5fOODR0TiU//cW7TlY2/+euWGzr6JqgbHbvistS/fKn31Y2kybGWZJW5Bnf/2pFi5q9M+Py32njhX3t8veruTpdL5PVzx4/V1YMW52i6swIOoltrEi8Gql2uQK9z6KfhRd/NuZPHMd5ReGb3XESgm92x0kIbc96437safJwzJwdVb1FB5FYgTbNbHbu1m2b0lQJhMstA8DmrbGvWcgZrXwMXWFkaCgai9F+qlKh9lNGZhzIb1RaAHSATLFTr7FIpa2rRhupZ5dsVnM1aolVNaryfOOmH0Vj7isOACPD8f2wSnvv2RNXoZlkBL7k6dxfv/ICZVMux+2nMnkdvFUq6RZVuUwc5FQu68pBaVp3xXw+6Rgx3ld6Hiz/W1LxZkhzZSdLZNl7vjF/4jjOKwrf7I6TEFre7CKSFpEnROSuxniuiDwkIitE5HYR0b+DOY4zbngp3+wXA9i3jOhXANwQQjgewA4AFx7KhTmOc2hpSaATkT4AbwdwLYBPymjNpzcD+MuGySIAXwDwD/s9D4AUBUBcTVlwV9zwA3VcCLHoUDZKV4VafN6sUQapSsKSGGV/WA20BJlB6vPekddCytQpupzT7l2x2GSVM+YAmXRWn7tOQSNZI+uOy1Tnivp+7B6MA18KeS3i/eL2ITX3wIP/Nxqn0loU4my5qhGcxD37Jk3q1uehU3/+I+cqm56eOPCmakQ0pUlo5XsI2CIiK8apYAhglJmYNcRZDryp1IzrUwmyWl1f65q749dRnBBfyyrHtpdWv9m/DuBS4MWQnikABkIIe1e8HsCsFs/lOM4RoOlmF5FzAGwJITRvVWkff5GIPCoij+7co78lHMdpD638Gv96AO8QkbMBFAB0A7gRQI+IZBrf7n0AdIlRACGEmwHcDADzjukdO7PecZzDStPNHkL4DIDPAICIvAnAp0II7xeRfwZwHoDbAJwP4I6m5wJQZ5+HfJmC0YJoiJJjakZFlSz5ZFYyhGrLYwQtDAzEATNWn3X2q7fv1BVWrJZMq1avj8Ynv0b3Q1dBNEa5kholSOwiLQAAsrn4+kNDWnsYKsV+9COPPahsLNLka1s+6vTpcUssq9x2PcTv0ZcveY++Vjr2o6tV/TpKNBe0PIAi9ToXI3mpZugzWSo/Xq0ZQTWZeI1WaycO6EoZSS4I5LMbfn02F+sautXU4QmquQyjYt0KjPrw3zmIczmOc5h5SeGyIYQHADzQ+PcqAAsP/ZIcxzkceASd4yQE3+yOkxDanvWmqnpQxY5rPvof1DF/c/Ut8TFGhhv3aMsZfd7LlI1kZcrVKSCjbogk6rYZJntGdJno+SfEfcPzWd1rju+PlYnG2VAjhmD54MOPx9fKW9eKx7mCtkkbmXnTpkyObYzqLVd/LA6W4vs6CpVOrmhhSyR+bWnr+0lllOk1D9N7ny/o5yNlBGKpCuVG73XOqCvkjIw6CsTKGqWs+b22Mh7TdFwqRfd+bH3Ov9kdJyn4ZnechOCb3XESQtt9dvaTOSFAjOD/3t7eaDwwoKvCliggomIESLBHyFVpAEB15bGqgJKzy2MAGBzU1UpymThceKSsbSZNnkw2OkLkyafj9kspw0flfuhWYhBX05lsVIGxKuBe/fHz4olgtDJibca613zfUobPTkFFll/NL79s3DMdrKSXw/oAAFTrpBkYAUQpCjKqGIlaBapmM1LSNinSR770Sx3Qle+Mr8/BW96f3XEc3+yOkxR8sztOQvDN7jgJoa0CXQi6xDH3pLbKK+/csSMaj4zokr+iggsM0Yqy1cpWZhxdf2TYyITKUylnI0AiGD26B3bF2XucYQcAWB1nCluVR3jKysybdlTc19w6D9/HL/7tu5VNxbhHtWosLgXjPctQFaCMIdDxmiqG0FmnCkS5jLaplElEM4J8MiSQVcs66ClllHfmPvfW92Odsu4yRrBUqcRBX/pafK/Txnk4iEYFmHkpacdxfLM7TkLwze44CaHNQTUBgcqI5ChpgNtBAcBXP/2BaHzp339f2QyPxAEq3KIIAIRerRVUw4EmlYr22bkKjlWpZeuWF9RcgdotZQxfn1++de6p06hyrZFkctVH3kE2xqV0lJGyyRlznLCRNfzEQMEwoa4DVvjFWlVqOanEajWVzcXfWVbts/IwtX8y/HqrAlIqQ8k6deMe0ftYM54ZDiarVnVQTY6SldJGIgwH0aSMdt1j4d/sjpMQfLM7TkLwze44CcE3u+MkhDYLdAKkSHSg0tJG0hsCuLqNJUrEB2YNAYZL/FpVaLh8r1XNpkrtfoaHdfZap9GSSQWWGOeeTsEw1vWv+qu4BVLNEL9YM6sZIl6OxT8jE8w6jpPTLOmNs8U6jEeNA4+soJZhCvzJGz3cSyWqAmMErPB9tDIVLa0rRd+HEqwS5VRK2tBda/TMWM/w1f8Sz3V0Gxl2tEjhsQfVOI7jm91xEoJvdsdJCG312UWALPmtdfLb0kZLXPZDrDZBV1z/w2i8Z08rLZn0Zx23lLYiNNgnTButniZTBVYA6CjGASJcXQcAPvfxd0XjuhEcFMiP5mQiAAg1ClgxgpUqNdZClInpA1bpehmjmg8H4wwb1WPypBmIVfGGrmXdM64mK8b94JZVdavtd0Xf65HAAVQ6OUUCaTaGP86BN8F4P7hyLPvj1pxlMxb+ze44CcE3u+MkBN/sjpMQfLM7TkIQS9w5bBcT2QpgLYCpAHRa2Pjm5bhm4OW5bl/zgXNMCGGa9YO2bvYXLyryaAhhQdsvfBC8HNcMvDzX7Ws+PPiv8Y6TEHyzO05COFKb/eYjdN2D4eW4ZuDluW5f82HgiPjsjuO0H/813nESQts3u4icJSLLRGSFiFze7uu3goh8V0S2iMjifeYmi8i9IrK88f9JR3KNjIjMFpH7RWSJiDwjIhc35sftukWkICIPi8iTjTVf3ZifKyIPNZ6R20VEJ7EfYUQkLSJPiMhdjfG4X3NbN7uMZux/E8DbAMwH8D4Rmd/ONbTIrQDOornLAdwXQpgH4L7GeDxRBXBJCGE+gNcB+Gjj3o7ndZcAvDmEcCqA0wCcJSKvA/AVADeEEI4HsAPAhUdwjWNxMYCl+4zH/Zrb/c2+EMCKEMKqEEIZwG0Azm1yTNsJIfwWwHaaPhfAosa/FwF4Z1sX1YQQQn8I4fHGv3dj9EGchXG87jDK3vTEbOO/AODNAH7UmB9XawYAEekD8HYAtzTGgnG+ZqD9m30WgHX7jNc35l4OTA8h9Df+vQnA9CO5mP0hInMAnA7gIYzzdTd+Hf4DgC0A7gWwEsBACC/mlo7HZ+TrAC7F/y/kNQXjf80u0B0IYfRPGOPyzxgiMgHAjwF8IoSwa9+fjcd1hxBqIYTTAPRh9De/E4/wkvaLiJwDYEsI4bEjvZaXSpsLTmIDgNn7jPsacy8HNotIbwihX0R6MfpNNK4QkSxGN/r3Qwg/aUyP+3UDQAhhQETuB3AmgB4RyTS+KcfbM/J6AO8QkbMBFAB0A7gR43vNANr/zf4IgHkN5TIH4L0A7mzzGg6UOwGc3/j3+QDuOIJrUTT8xu8AWBpCuH6fH43bdYvINBHpafy7COCtGNUa7gdwXsNsXK05hPCZEEJfCGEORp/ffw0hvB/jeM0vEkJo638AzgbwHEZ9syvbff0W1/gDAP0AKhj1vy7EqF92H4DlAH4NYPKRXiet+Q0Y/RX9KQB/aPx39nheN4BTADzRWPNiAJ9rzB8L4GEAKwD8M4D8kV7rGOt/E4C7Xi5r9gg6x0kILtA5TkLwze44CcE3u+MkBN/sjpMQfLM7TkLwze44CcE3u+MkBN/sjpMQ/h8GRt3oel0mbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reset the environment\n",
    "states = env.reset()\n",
    "num_agents = 1\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape, state_size))\n",
    "print('The state for the agent looks like:', plt.imshow(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5, train_mode=False):\n",
    "\n",
    "    def to_tensor(x, dtype=np.float32):\n",
    "        return torch.from_numpy(np.array(x).astype(dtype)).to(device)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "    value_list=[]\n",
    "    done_list=[]\n",
    "\n",
    "    env_info = envs.reset()\n",
    "\n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        action = np.random.randn(1, action_size)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    for t in range(tmax):\n",
    "        next_state=env_info\n",
    "        states = to_tensor(next_state)\n",
    "        action_est, values = policy(states.reshape(1,-1))\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        log_probs = torch.sum(log_probs, dim=-1).detach()\n",
    "        values = values.detach()\n",
    "        actions = actions.detach()\n",
    "\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        rewards = to_tensor(env_info.rewards)\n",
    "        dones = to_tensor(env_info.local_done, dtype=np.uint8)\n",
    "\n",
    "        state_list.append(states.unsqueeze(0))\n",
    "        prob_list.append(log_probs.unsqueeze(0))\n",
    "        action_list.append(actions.unsqueeze(0))\n",
    "        reward_list.append(rewards.unsqueeze(0))\n",
    "        value_list.append(values.unsqueeze(0))\n",
    "        done_list.append(dones.unsqueeze(0))\n",
    "        #if np.any(dones.cpu().numpy()):\n",
    "        if np.any(dones.numpy()):\n",
    "            env_info = envs.reset()\n",
    "\n",
    "    state_list = torch.cat(state_list, dim=0)\n",
    "    prob_list = torch.cat(prob_list, dim=0)\n",
    "    action_list = torch.cat(action_list, dim=0)\n",
    "    reward_list = torch.cat(reward_list, dim=0)\n",
    "    value_list = torch.cat(value_list, dim=0)\n",
    "    done_list = torch.cat(done_list, dim=0)\n",
    "    return prob_list, state_list, action_list, reward_list, value_list, done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(rewards, values, dones):\n",
    "    n_step = len(rewards)\n",
    "    n_agent = len(rewards[0])\n",
    "\n",
    "    # Create empty buffer\n",
    "    GAE = torch.zeros(n_step,n_agent).float().to(device)\n",
    "    returns = torch.zeros(n_step,n_agent).float().to(device)\n",
    "\n",
    "    # Set start values\n",
    "    GAE_current = torch.zeros(n_agent).float().to(device)\n",
    "\n",
    "    TAU = 0.95\n",
    "    discount = 0.99\n",
    "    values_next = values[-1].detach()\n",
    "    returns_current = values[-1].detach()\n",
    "    for irow in reversed(range(n_step)):\n",
    "        values_current = values[irow]\n",
    "        rewards_current = rewards[irow]\n",
    "        gamma = discount * (1. - dones[irow].float())\n",
    "\n",
    "        # Calculate TD Error\n",
    "        td_error = rewards_current + gamma * values_next - values_current\n",
    "        # Update GAE, returns\n",
    "        GAE_current = td_error + gamma * TAU * GAE_current\n",
    "        returns_current = rewards_current + gamma * returns_current\n",
    "        # Set GAE, returns to buffer\n",
    "        GAE[irow] = GAE_current\n",
    "        returns[irow] = returns_current\n",
    "\n",
    "        values_next = values_current\n",
    "\n",
    "    return GAE, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "An actor-critic structure with continuous action space is used for this project. The policy consists of 3 parts, a shared hidden layers, actor, and critic.\n",
    "The actor layer outputs the mean value of a normal distribution, from which the agent's action is sampled. The critic layer yields the value function.\n",
    "\n",
    "- Shared layer:\n",
    "```\n",
    "Input State(33) -> Dense(128) -> LeakyReLU -> Dense(128) -> LeakyReLU*\n",
    "```\n",
    "- Actor and Critic layers:\n",
    "```\n",
    "LeakyRelu* -> Dense(64) -> LeakyRelu -> Dense(4)-> tanh -> Actor's output\n",
    "LeakyReLU* -> Dense(64) -> LeakyRelu -> Dense(1) -> Critic's output\n",
    "```\n",
    "\n",
    "### Model update using PPO/GAE\n",
    "The hyperparameters used during training are:\n",
    "\n",
    "Parameter | Value | Description\n",
    "------------ | ------------- | -------------\n",
    "Number of Agents | 20 | Number of agents trained simultaneously\n",
    "Episodes | 2000 | Maximum number of training episodes\n",
    "tmax | 1000 | Maximum number of steps per episode\n",
    "Epochs | 10 | Number of training epoch per batch sampling\n",
    "Batch size | 128*20 | Size of batch taken from the accumulated  trajectories\n",
    "Discount (gamma) | 0.99 | Discount rate \n",
    "Epsilon | 0.1 | Ratio used to clip r = new_probs/old_probs during training\n",
    "Gradient clip | 10.0 | Maximum gradient norm \n",
    "Beta | 0.01 | Entropy coefficient \n",
    "Tau | 0.95 | tau coefficient in GAE\n",
    "Learning rate | 2e-4 | Learning rate \n",
    "Optimizer | Adam | Optimization method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=ActorCritic(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              shared_layers=[128, 64],\n",
    "              critic_hidden_layers=[],\n",
    "              actor_hidden_layers=[],\n",
    "              init_type='xavier-uniform',\n",
    "              seed=0).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 6912], m2: [48 x 128] at /opt/conda/conda-bld/pytorch_1525909934016/work/aten/src/TH/generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9713d92e264f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                                                        \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                                        \u001b[0mnrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                                                                        train_mode=True)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mavg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-01335d3812d7>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(envs, policy, tmax, nrand, train_mode)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0maction_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/od/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5206690b0034>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_multi_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mv_hid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5206690b0034>\u001b[0m in \u001b[0;36mapply_multi_layer\u001b[0;34m(layers, x, f)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_multi_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/od/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/od/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/od/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 6912], m2: [48 x 128] at /opt/conda/conda-bld/pytorch_1525909934016/work/aten/src/TH/generic/THTensorMath.c:2033"
     ]
    }
   ],
   "source": [
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "discount = 0.993\n",
    "epsilon = 0.07\n",
    "beta = .01\n",
    "opt_epoch = 10\n",
    "episode = 2000\n",
    "batch_size = 1\n",
    "tmax = 10 #env episode steps\n",
    "save_scores = []\n",
    "print_per_n = min(10,episode/10)\n",
    "counter = 0\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for e in range(episode):\n",
    "    policy.eval()\n",
    "    old_probs_lst, states_lst, actions_lst, rewards_lst, values_lst, dones_list = collect_trajectories(envs=env,\n",
    "                                                                                                       policy=policy,\n",
    "                                                                                                       tmax=tmax,\n",
    "                                                                                                       nrand = 0,\n",
    "                                                                                                       train_mode=True)\n",
    "\n",
    "    avg_score = rewards_lst.sum(dim=0).mean().item()\n",
    "    scores_window.append(avg_score)\n",
    "    save_scores.append(avg_score)\n",
    "    \n",
    "    gea, target_value = calc_returns(rewards = rewards_lst,\n",
    "                                     values = values_lst,\n",
    "                                     dones=dones_list)\n",
    "    gea = (gea - gea.mean()) / (gea.std() + 1e-8)\n",
    "\n",
    "    policy.train()\n",
    "\n",
    "    # cat all agents\n",
    "    def concat_all(v):\n",
    "        if len(v.shape) == 3:\n",
    "            return v.reshape([-1, v.shape[-1]])\n",
    "        return v.reshape([-1])\n",
    "\n",
    "    old_probs_lst = concat_all(old_probs_lst)\n",
    "    states_lst = concat_all(states_lst)\n",
    "    actions_lst = concat_all(actions_lst)\n",
    "    rewards_lst = concat_all(rewards_lst)\n",
    "    values_lst = concat_all(values_lst)\n",
    "    gea = concat_all(gea)\n",
    "    target_value = concat_all(target_value)\n",
    "\n",
    "    # gradient ascent step\n",
    "    n_sample = len(old_probs_lst)//batch_size\n",
    "    idx = np.arange(len(old_probs_lst))\n",
    "    np.random.shuffle(idx)\n",
    "    for epoch in range(opt_epoch):\n",
    "        for b in range(n_sample):\n",
    "            ind = idx[b*batch_size:(b+1)*batch_size]\n",
    "            g = gea[ind]\n",
    "            tv = target_value[ind]\n",
    "            actions = actions_lst[ind]\n",
    "            old_probs = old_probs_lst[ind]\n",
    "\n",
    "            action_est, values = policy(states_lst[ind])\n",
    "            sigma = nn.Parameter(torch.zeros(action_size))\n",
    "            dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            log_probs = torch.sum(log_probs, dim=-1)\n",
    "            entropy = torch.sum(dist.entropy(), dim=-1)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_probs)\n",
    "            ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            L_CLIP = torch.mean(torch.min(ratio*g, ratio_clipped*g))\n",
    "            # entropy bonus\n",
    "            S = entropy.mean()\n",
    "            # squared-error value function loss\n",
    "            L_VF = 0.5 * (tv - values).pow(2).mean()\n",
    "            # clipped surrogate\n",
    "            L = -(L_CLIP - L_VF + beta*S)\n",
    "            optimizer.zero_grad()\n",
    "            # This may need retain_graph=True on the backward pass\n",
    "            # as pytorch automatically frees the computational graph after\n",
    "            # the backward pass to save memory\n",
    "            # Without this, the chain of derivative may get lost\n",
    "            L.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            del(L)\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.998\n",
    "    \n",
    "    # display some progress every n iterations\n",
    "    if (e+1)%print_per_n ==0 :\n",
    "        print(\"Episode: {0:d}, average score: {1:.2f}\".format(e+1,np.mean(scores_window)), end=\"\\n\")\n",
    "    else:\n",
    "        print(\"Episode: {0:d}, score: {1:.2f}\".format(e+1, avg_score), end=\"\\r\")\n",
    "    if np.mean(scores_window)<5.0:\n",
    "        counter = 0# stop if any of the trajectories is done to have retangular lists\n",
    "    if e>=25 and np.mean(scores_window)>30.0:\n",
    "        print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e+1, np.mean(scores_window)))\n",
    "        break\n",
    "\n",
    "\n",
    "print('Average Score: {:.2f}'.format(np.mean(scores_window)))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time: {}\".format(timedelta(seconds=elapsed)))\n",
    "print(\"Saving checkpoint!\")\n",
    "# save your policy!\n",
    "torch.save(policy.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(envs, policy, tmax=1000):\n",
    "    reward_list=[]\n",
    "    env_info = envs.reset(train_mode=False)[brain_name]\n",
    "    for t in range(tmax):\n",
    "        states = torch.from_numpy(env_info.vector_observations).float().to(device)\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        reward = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        reward_list.append(np.mean(reward))\n",
    "\n",
    "        # stop if any of the trajectories is done to have retangular lists\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "      #print(state)\n",
    "      old_Q = Q[state][tuple(np.round(actions[i].flat))]\n",
    "      Q[state][tuple(np.round(actions[i].flat))] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA, height_hack_prob=0.9):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "      env.render(mode='human')\n",
    "      state_m = tuple(np.round(state.flat))\n",
    "      action = np.random.choice(np.arange(nA), p=get_probs(Q[state_m], epsilon, nA)) if state_m in Q else action_space.sample()\n",
    "      if np.random.random() < height_hack_prob:\n",
    "        action[2] = -1\n",
    "      if state_m in Q:\n",
    "        print(\"*******************From Q!*************************\") \n",
    "      action_r = np.round(action)\n",
    "      next_state, reward, done, info = env.step(action_r)\n",
    "      episode.append((state_m, action_r, reward))\n",
    "      state = next_state\n",
    "      if done:\n",
    "        break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=0.95, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    nA = action_space.shape[0]\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: np.zeros(nA)))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(episode, Q, alpha, gamma)\n",
    "        # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = mc_control(env, num_episodes=3, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
